{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pint out versions of sklearn \n",
    "import sklearn\n",
    "import pandas as pd\n",
    "print(sklearn.__version__) # 1.3.1\n",
    "print(pd.__version__) # 2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictive analyses\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.0)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# Create directory for saving the best models\n",
    "os.makedirs(\"best_models\", exist_ok=True)\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "def run_ML(df_dsp_back_ho_norm_new, df_dsp_back_ba_norm_new, genes_of_interest, scatter_size):\n",
    "    # Prepare cross-validation, plot setup, and other initializations remain the same...\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    models = [Ridge(), RandomForestRegressor(), SVR()]\n",
    "    params = [\n",
    "        {\n",
    "            \"alpha\": [1e-08, 1e-07, 1e-6, 1e-5, 1e-4, 1e-3, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "        },\n",
    "\n",
    "        # random forest parameters\n",
    "        {\n",
    "        # Parameters for Random Forest\n",
    "        \"n_estimators\": [100, 300, 500, 1000],\n",
    "        \"max_depth\": [None, 3, 5, 7, 9],  # Including None as an option\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"bootstrap\": [True, False]\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            \"kernel\": [\"linear\", \"rbf\"],\n",
    "            \"gamma\": [0.01, 0.1, 1, 10],\n",
    "            \"epsilon\": [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "        },\n",
    "\n",
    "    ]\n",
    "    model_names = [\"Ridge Regression\", \"Random Forest\", \"SVM\"]\n",
    "\n",
    "    # For each model\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        param = params[i]\n",
    "        model_name = model_names[i]\n",
    "        mses = []\n",
    "        maes = []\n",
    "        r2s = []\n",
    "        evss = []\n",
    "\n",
    "        model_path = f'best_models_013024/{model_name.replace(\" \", \"_\").lower()}_{genes_of_interest.replace(\"/\", \"_\")}.pkl'\n",
    "\n",
    "        if os.path.exists(model_path):  # If the model already exists, load it\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                best_model = pickle.load(f)\n",
    "            print(\"Read saved models\")\n",
    "        else:  # Otherwise, perform grid search to find the best model\n",
    "            grid = GridSearchCV(\n",
    "                model, param, cv=kfold, scoring=\"neg_mean_absolute_error\", n_jobs=-1\n",
    "            )\n",
    "            grid.fit(df_dsp_back_ho_norm_new, df_dsp_back_ba_norm_new)\n",
    "            best_model = grid.best_estimator_\n",
    "\n",
    "            # Save the best model\n",
    "            with open(model_path, \"wb\") as f:\n",
    "                pickle.dump(best_model, f)\n",
    "\n",
    "        residuals = []\n",
    "        preds = []  # List to store all predictions\n",
    "\n",
    "        feature_importances = [] \n",
    "        # For each split\n",
    "        for j, (train, test) in enumerate(kfold.split(df_dsp_back_ho_norm_new)):\n",
    "            X_train, X_test = (\n",
    "                df_dsp_back_ho_norm_new.iloc[train],\n",
    "                df_dsp_back_ho_norm_new.iloc[test],\n",
    "            )\n",
    "            y_train, y_test = (\n",
    "                df_dsp_back_ba_norm_new.iloc[train],\n",
    "                df_dsp_back_ba_norm_new.iloc[test],\n",
    "            )\n",
    "\n",
    "            # Fit and predict\n",
    "            best_model.fit(X_train, y_train)\n",
    "            pred = best_model.predict(X_test)\n",
    "            preds.extend(pred)  # Add predictions to the list\n",
    "            \n",
    "            # Save feature importances for Random Forest model\n",
    "            if model_name == \"Ridge Regression\":\n",
    "                feature_importances.append(best_model.coef_ )\n",
    "\n",
    "            mse = mean_squared_error(y_test, pred)\n",
    "            mses.append(mse)\n",
    "\n",
    "            mae = mean_absolute_error(y_test, pred)\n",
    "            maes.append(mae)\n",
    "\n",
    "            r2 = r2_score(y_test, pred)\n",
    "            r2s.append(r2)\n",
    "\n",
    "            evs = explained_variance_score(y_test, pred)\n",
    "            evss.append(evs)\n",
    "\n",
    "            residuals.extend(\n",
    "                y_test - pred\n",
    "            )  # Calculate residuals and add them to the list\n",
    "\n",
    "            axs[i].scatter(\n",
    "                y_test,\n",
    "                pred,\n",
    "                alpha=0.6,\n",
    "                label=\"Fold {}; MAE: {:.2f}\".format(j + 1, mae),\n",
    "                s=scatter_size,\n",
    "            )\n",
    "            \n",
    "        axs[i].set_title(\n",
    "            \"{} - Avg MAE: {:.3f}, Avg EVS: {:.3f}, Avg R2: {: 3f}\".format(\n",
    "                model_name, np.mean(maes), np.mean(evss), np.mean(r2s)\n",
    "            )\n",
    "        )\n",
    "        print(\"R2: \", np.mean(r2s))\n",
    "\n",
    "        axs[i].plot(\n",
    "            [df_dsp_back_ba_norm_new.min(), df_dsp_back_ba_norm_new.max()],\n",
    "            [df_dsp_back_ba_norm_new.min(), df_dsp_back_ba_norm_new.max()],\n",
    "            \"k--\",\n",
    "        )\n",
    "        axs[i].set_xlabel(\"True Values (\" + genes_of_interest + \")\", size = 16)\n",
    "        axs[i].set_ylabel(\"Predictions (\" + genes_of_interest + \")\", size = 16)\n",
    "        axs[i].legend()\n",
    "        \n",
    "        if model_name == \"Ridge Regression\":\n",
    "            # Calculate average feature importance and rank the features\n",
    "            avg_feat_imp = np.mean(feature_importances, axis=0)\n",
    "            ranked_features = np.argsort(avg_feat_imp)[::-1]\n",
    "\n",
    "            # Save feature importances to a DataFrame\n",
    "            feat_imp_df = pd.DataFrame({\n",
    "                \"Feature\": df_dsp_back_ho_norm_new.columns,\n",
    "                \"Importance\": avg_feat_imp\n",
    "            })\n",
    "            feat_imp_df['Importance'] = np.abs(feat_imp_df['Importance'])\n",
    "            feat_imp_df.to_csv(f\"feat_imp/{model_name.replace(' ', '_').lower()}_{genes_of_interest.replace('/', '_')}_feature_importances.csv\", index=False)\n",
    "\n",
    "            # Select top 30 important features\n",
    "            top_30_features = df_dsp_back_ho_norm_new.columns[ranked_features[:100]]\n",
    "\n",
    "            # Retrain the model using top 30 important features\n",
    "            best_model.fit(df_dsp_back_ho_norm_new[top_30_features], df_dsp_back_ba_norm_new)\n",
    "\n",
    "            # Evaluate the model using top 30 important features\n",
    "            scores = -1 * cross_val_score(best_model, df_dsp_back_ho_norm_new[top_30_features], df_dsp_back_ba_norm_new, \n",
    "                                          cv=kfold, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "            print(f\"Average MAE for {model_name} with top 100 features: {np.mean(scores):.3f}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "for genes_of_interest in [\"rplL\"]:\n",
    "\n",
    "    selected_rois = df_dsp_back_ba.loc[genes_of_interest][\n",
    "        df_dsp_back_ba.loc[genes_of_interest] >= 0\n",
    "    ].index.tolist()\n",
    "    \n",
    "    selected_rois = list(set(meta[meta.Group.isin(['WT_PA'])].index.tolist()) & set(selected_rois))\n",
    "    \n",
    "    # df_dsp_back_ba_norm_new = df_dsp_back_ba[selected_rois].loc[genes_of_interest]\n",
    "    df_dsp_back_ba_norm_new = np.log2(df_dsp_back_ba[selected_rois].loc[genes_of_interest] + 1)\n",
    "    \n",
    "    print(\"bac_shape:\", df_dsp_back_ba_norm_new.shape)\n",
    "\n",
    "    # normalized \"df_dsp_back_ho_select\" using StandardScaler >> df_dsp_back_ho_norm_new (Host gene expression)\n",
    "    # df_dsp_back_ho_norm_new's shape is (43, 19962), sharing same columns with \"df_dsp_back_ba_norm_new\"\n",
    "    scaler = StandardScaler()\n",
    "    df_dsp_back_ho_select = df_dsp_back_ho[df_dsp_back_ba_norm_new.index]\n",
    "\n",
    "    df_dsp_back_ho_norm_new = scaler.fit_transform(df_dsp_back_ho_select.T)\n",
    "    df_dsp_back_ho_norm_new = pd.DataFrame(\n",
    "        df_dsp_back_ho_norm_new,\n",
    "        index=df_dsp_back_ho_select.columns,\n",
    "        columns=df_dsp_back_ho_select.index,\n",
    "    )\n",
    "    print(\"ho_shape:\", df_dsp_back_ho_norm_new.shape)\n",
    "\n",
    "    run_ML(df_dsp_back_ho_norm_new, df_dsp_back_ba_norm_new, genes_of_interest, scatter_size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "random_state_seed = 40\n",
    "def evaluate_features_performance(df_dsp_back_ho_norm_new, df_dsp_back_ba_norm_new, top_n_features_list, genes_of_interest):\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=random_state_seed)\n",
    "    model_name = \"Ridge Regression\"\n",
    "    best_r2_score = -np.inf\n",
    "    best_n_features = None\n",
    "    r2_scores_list = []\n",
    "\n",
    "    # Hyperparameters for Ridge Regression\n",
    "    params = {\n",
    "        \"alpha\": [1e-08, 1e-07, 1e-6, 1e-5, 1e-4, 1e-3, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "\n",
    "    for n in top_n_features_list:\n",
    "        file_path = f\"feat_imp/{model_name.replace(' ', '_').lower()}_{genes_of_interest.replace('/', '_')}_feature_importances.csv\"\n",
    "        feat_imp_df = pd.read_csv(file_path)\n",
    "        feat_imp_df['Importance'] = np.abs(feat_imp_df['Importance'])\n",
    "        top_features = feat_imp_df.sort_values(by='Importance', ascending=False).head(n)['Feature'].values\n",
    "\n",
    "        r2_scores = []\n",
    "\n",
    "        # Create Ridge model and GridSearchCV\n",
    "        ridge = Ridge()\n",
    "        grid = GridSearchCV(ridge, params, cv=kfold, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "        grid.fit(df_dsp_back_ho_norm_new[top_features], df_dsp_back_ba_norm_new)\n",
    "        best_model = grid.best_estimator_\n",
    "\n",
    "        for train_idx, test_idx in kfold.split(df_dsp_back_ho_norm_new):\n",
    "            X_train, X_test = df_dsp_back_ho_norm_new.iloc[train_idx][top_features], df_dsp_back_ho_norm_new.iloc[test_idx][top_features]\n",
    "            y_train, y_test = df_dsp_back_ba_norm_new.iloc[train_idx], df_dsp_back_ba_norm_new.iloc[test_idx]\n",
    "\n",
    "            best_model.fit(X_train, y_train)\n",
    "\n",
    "            pred = best_model.predict(X_test)\n",
    "            r2_scores.append(r2_score(y_test, pred))\n",
    "\n",
    "        # print best model's parameters\n",
    "        # print(f\"Best model's parameters: {best_model.get_params()}; \" + f\"R²: {np.mean(r2_scores):.3f}; Top {n} features\")\n",
    "\n",
    "        avg_r2_score = np.mean(r2_scores)\n",
    "        r2_scores_list.append(avg_r2_score)\n",
    "        if avg_r2_score > best_r2_score:\n",
    "            best_r2_score = avg_r2_score\n",
    "            best_n_features = n\n",
    "\n",
    "    return best_n_features, r2_scores_list\n",
    "\n",
    "def run_ML_top_features(df_dsp_back_ho_norm_new, df_dsp_back_ba_norm_new, genes_of_interest, scatter_size, top_n_features, axs):\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=random_state_seed)\n",
    "\n",
    "    models = [Ridge()]\n",
    "    model_names = [\"Ridge Regression\"]\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        model_name = model_names[i]\n",
    "\n",
    "        # Define hyperparameters for Ridge Regression\n",
    "        params = {\n",
    "            \"alpha\": [1e-08, 1e-07, 1e-6, 1e-5, 1e-4, 1e-3, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        }\n",
    "\n",
    "        # Grid search to find best parameters\n",
    "        ridge = Ridge()\n",
    "        grid = GridSearchCV(ridge, params, cv=kfold, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "        grid.fit(df_dsp_back_ho_norm_new[top_n_features], df_dsp_back_ba_norm_new)\n",
    "        best_model = grid.best_estimator_\n",
    "\n",
    "        # best_model = pickle.load(open(f'best_models_013024/{model_name.replace(\" \", \"_\").lower()}_{genes_of_interest.replace(\"/\", \"_\")}.pkl', 'rb'))\n",
    "        \n",
    "        scores = []\n",
    "        r2_scores = []\n",
    "        ev_scores = []\n",
    "\n",
    "        for j, (train, test) in enumerate(kfold.split(df_dsp_back_ho_norm_new[top_n_features])):\n",
    "            X_train, X_test = (\n",
    "                df_dsp_back_ho_norm_new[top_n_features].iloc[train],\n",
    "                df_dsp_back_ho_norm_new[top_n_features].iloc[test],\n",
    "            )\n",
    "            y_train, y_test = (\n",
    "                df_dsp_back_ba_norm_new.iloc[train],\n",
    "                df_dsp_back_ba_norm_new.iloc[test],\n",
    "            )\n",
    "\n",
    "            best_model.fit(X_train, y_train)\n",
    "            # print best model's parameters\n",
    "            pred = best_model.predict(X_test)\n",
    "\n",
    "            mae = mean_absolute_error(y_test, pred)\n",
    "            scores.append(mae)\n",
    "            r2_scores.append(r2_score(y_test, pred))\n",
    "            ev_scores.append(explained_variance_score(y_test, pred))\n",
    "\n",
    "            axs.scatter(\n",
    "                y_test,\n",
    "                pred,\n",
    "                alpha=0.6,\n",
    "                label=\"ROI_Fold {}; MAE: {:.2f}\".format(j + 1, mae),\n",
    "                s=scatter_size,\n",
    "            )\n",
    "        \n",
    "        print(np.mean(r2_scores))\n",
    "\n",
    "        axs.set_title(\n",
    "            \"{} - Avg MAE: {:.3f}, Avg EVS: {:.3f}; R²: {:.3f}; Top {} feat\".format(\n",
    "                'RR', np.mean(scores), np.mean(ev_scores), np.mean(r2_scores), len(top_n_features)\n",
    "            )\n",
    "        )\n",
    "        axs.set_xlabel(\"True Values (\" + genes_of_interest + \")\", size=16)\n",
    "        axs.set_ylabel(\"Predictions (\" + genes_of_interest + \")\", size=16)\n",
    "        axs.legend()\n",
    "        axs.plot(\n",
    "            [df_dsp_back_ba_norm_new.min(), df_dsp_back_ba_norm_new.max()],\n",
    "            [df_dsp_back_ba_norm_new.min(), df_dsp_back_ba_norm_new.max()],\n",
    "            \"k--\",\n",
    "        )\n",
    "\n",
    "n_rows = 1\n",
    "n_cols = 1\n",
    "\n",
    "genes = [\"rplL\"]\n",
    "n_features_list = [10, 20, 30, 80, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000,\n",
    "                   2000, 3000, 4000, 5000, 7500, 10000, 11000, 12000, 15000, 19962]  # Adjust as needed\n",
    "\n",
    "selected_rois = df_dsp_back_ba.loc['rplL'][\n",
    "        df_dsp_back_ba.loc['rplL'] >= 0\n",
    "    ].index.tolist()\n",
    "    \n",
    "selected_rois = list(set(meta[meta.Group.isin(['WT_PA'])].index.tolist()) & set(selected_rois))\n",
    "\n",
    "# df_dsp_back_ba_norm_new = df_dsp_back_ba[selected_rois].loc[genes_of_interest]\n",
    "df_dsp_back_ba_norm_new = np.log2(df_dsp_back_ba[selected_rois].loc['rplL'] + 1)\n",
    "\n",
    "print(\"bac_shape:\", df_dsp_back_ba_norm_new.shape)\n",
    "\n",
    "# normalized \"df_dsp_back_ho_select\" using StandardScaler >> df_dsp_back_ho_norm_new (Host gene expression)\n",
    "# df_dsp_back_ho_norm_new's shape is (43, 19962), sharing same columns with \"df_dsp_back_ba_norm_new\"\n",
    "scaler = StandardScaler()\n",
    "df_dsp_back_ho_select = df_dsp_back_ho[df_dsp_back_ba_norm_new.index]\n",
    "\n",
    "df_dsp_back_ho_norm_new = scaler.fit_transform(df_dsp_back_ho_select.T)\n",
    "df_dsp_back_ho_norm_new = pd.DataFrame(\n",
    "    df_dsp_back_ho_norm_new,\n",
    "    index=df_dsp_back_ho_select.columns,\n",
    "    columns=df_dsp_back_ho_select.index,\n",
    ")\n",
    "\n",
    "best_n, r2_scores = evaluate_features_performance(df_dsp_back_ho_norm_new, df_dsp_back_ba_norm_new, n_features_list, 'rplL')\n",
    "\n",
    "# Find the number of features where R2 first exceeds 0.8\n",
    "above_08_indices = [i for i, r2 in enumerate(r2_scores) if r2 > 0.8]\n",
    "if above_08_indices:\n",
    "    index_first_above_08 = above_08_indices[0]\n",
    "    n_features_first_above_08 = n_features_list[index_first_above_08]\n",
    "    r2_first_above_08 = r2_scores[index_first_above_08]\n",
    "else:\n",
    "    index_first_above_08 = None\n",
    "    n_features_first_above_08 = None\n",
    "    r2_first_above_08 = None\n",
    "\n",
    "# Find the number of features with the highest R2 score\n",
    "max_r2 = max(r2_scores)\n",
    "index_max_r2 = r2_scores.index(max_r2)\n",
    "best_n_features = n_features_list[index_max_r2]\n",
    "\n",
    "# Plotting R2 scores vs Number of features\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(n_features_list, r2_scores, marker='x', label='R² Score', alpha=0.6)\n",
    "\n",
    "# Highlight the maximum point\n",
    "plt.scatter(best_n_features, max_r2, color='red', s=50, marker='o',\n",
    "             label=f'Max R²={max_r2:.3f} with {best_n_features} features')\n",
    "\n",
    "# Highlight the first point where R2 is above 0.8\n",
    "if index_first_above_08 is not None:\n",
    "    plt.scatter(n_features_first_above_08, r2_first_above_08, color='blue', s=50, marker='o',\n",
    "                label=f'First R²>0.8 at {n_features_first_above_08} features')\n",
    "\n",
    "plt.xlabel('Number of Features (Mouse Genes)')\n",
    "plt.ylabel('Average R² Score')\n",
    "plt.title('R² Score vs Number of Mouse Genes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "ranked_features_all = []\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(5, 5))\n",
    "\n",
    "for idx, genes_of_interest in enumerate(tqdm(genes)):\n",
    "    selected_rois = df_dsp_back_ba.loc[genes_of_interest][\n",
    "        df_dsp_back_ba.loc[genes_of_interest] >= 0\n",
    "    ].index.tolist()\n",
    "    selected_rois = list(set(meta[meta.Group.isin(['WT_PA'])].index.tolist()) & set(selected_rois))\n",
    "    \n",
    "    # df_dsp_back_ba_norm_new = df_dsp_back_ba[selected_rois].loc[genes_of_interest]\n",
    "    df_dsp_back_ba_norm_new = np.log2(df_dsp_back_ba[selected_rois].loc[genes_of_interest] + 1)\n",
    "    print(\"bac_shape:\", df_dsp_back_ba_norm_new.shape)\n",
    "\n",
    "    # normalized \"df_dsp_back_ho_select\" using StandardScaler >> df_dsp_back_ho_norm_new (Host gene expression)\n",
    "    # df_dsp_back_ho_norm_new's shape is (43, 19962), sharing same columns with \"df_dsp_back_ba_norm_new\"\n",
    "    scaler = StandardScaler()\n",
    "    df_dsp_back_ho_select = df_dsp_back_ho[df_dsp_back_ba_norm_new.index]\n",
    "    \n",
    "    df_dsp_back_ho_norm_new = scaler.fit_transform(df_dsp_back_ho_select.T)\n",
    "    df_dsp_back_ho_norm_new = pd.DataFrame(\n",
    "        df_dsp_back_ho_norm_new,\n",
    "        index=df_dsp_back_ho_select.columns,\n",
    "        columns=df_dsp_back_ho_select.index,\n",
    "    )\n",
    "    print(\"ho_shape:\", df_dsp_back_ho_norm_new.shape)\n",
    "    \n",
    "    rf_model_name = 'Ridge Regression'\n",
    "    avg_feat_imp_file = f\"feat_imp/{rf_model_name.replace(' ', '_').lower()}_{genes_of_interest.replace('/', '_')}_feature_importances.csv\"\n",
    "    feat_imp_df = pd.read_csv(avg_feat_imp_file)\n",
    "    feat_imp_df['Importance'] = np.abs(feat_imp_df['Importance'])\n",
    "    ranked_features = feat_imp_df.sort_values(by='Importance', ascending=False).head(best_n)['Feature'].values\n",
    "    ranked_features_all.append(ranked_features)\n",
    "    run_ML_top_features(df_dsp_back_ho_norm_new, df_dsp_back_ba_norm_new, genes_of_interest, scatter_size=40, top_n_features=ranked_features, axs=axs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "meta_wt = meta[meta.Group.str.contains('WT_PA')]\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(\"correlation_norm_013124\"):\n",
    "    os.makedirs(\"correlation_norm_013124\")\n",
    "\n",
    "correlation_file = \"correlation_norm_013124/correlation_matrix_wt.csv\"\n",
    "p_value_file = \"correlation_norm_013124/p_value_matrix_wt.csv\"\n",
    "fdr_p_value_file = \"correlation_norm_013124/fdr_p_value_matrix_wt.csv\"\n",
    "\n",
    "threshold = 0  \n",
    "if os.path.exists(correlation_file) and os.path.exists(p_value_file):\n",
    "    \n",
    "    # Normalize expression values (e.g., log transformation)\n",
    "    df_dsp_back_ho_norm = df_dsp_back_ho.apply(lambda x: np.log2(x + 1))[meta_wt[meta_wt.index.isin(set(df_dsp_back_ba.columns) & set(df_dsp_back_ho.columns))].index]\n",
    "    df_dsp_back_ba_norm = df_dsp_back_ba.apply(lambda x: np.log2(x + 1))[meta_wt[meta_wt.index.isin(set(df_dsp_back_ba.columns) & set(df_dsp_back_ho.columns))].index]\n",
    "    \n",
    "    # Filter out lowly expressed genes (optional)\n",
    "    df_dsp_back_ho_norm = df_dsp_back_ho_norm[df_dsp_back_ho_norm.apply(lambda x: x >= threshold).any(axis=1)]\n",
    "    df_dsp_back_ba_norm = df_dsp_back_ba_norm[df_dsp_back_ba_norm.apply(lambda x: x >= threshold).any(axis=1)]\n",
    "    \n",
    "    print(df_dsp_back_ho_norm.shape, df_dsp_back_ba_norm.shape)\n",
    "    \n",
    "    # Load the matrices from the saved files\n",
    "    correlation_matrix = np.loadtxt(correlation_file, delimiter=\",\")\n",
    "    p_value_matrix = np.loadtxt(p_value_file, delimiter=\",\")\n",
    "    \n",
    "    # Handle potential nan values in p_value_matrix\n",
    "    p_value_matrix = np.nan_to_num(p_value_matrix, nan=1.0)\n",
    "    \n",
    "    # Apply FDR correction\n",
    "    p_value_matrix = p_value_matrix.flatten()\n",
    "    _, fdr_corrected_p_values, _, _ = multipletests(p_value_matrix, method='fdr_bh')\n",
    "    fdr_corrected_p_values = fdr_corrected_p_values.reshape(correlation_matrix.shape)\n",
    "    \n",
    "    # Save the FDR corrected p-values\n",
    "    np.savetxt(fdr_p_value_file, fdr_corrected_p_values, delimiter=\",\")\n",
    "else:\n",
    "    # Normalize expression values (e.g., log transformation)\n",
    "    df_dsp_back_ho_norm = df_dsp_back_ho.apply(lambda x: np.log2(x + 1))[meta_wt[meta_wt.index.isin(set(df_dsp_back_ba.columns) & set(df_dsp_back_ho.columns))].index]\n",
    "    df_dsp_back_ba_norm = df_dsp_back_ba.apply(lambda x: np.log2(x + 1))[meta_wt[meta_wt.index.isin(set(df_dsp_back_ba.columns) & set(df_dsp_back_ho.columns))].index]\n",
    "\n",
    "    # Filter out lowly expressed genes (optional)\n",
    "    df_dsp_back_ho_norm = df_dsp_back_ho_norm[df_dsp_back_ho_norm.apply(lambda x: x >= threshold).any(axis=1)]\n",
    "    df_dsp_back_ba_norm = df_dsp_back_ba_norm[df_dsp_back_ba_norm.apply(lambda x: x >= threshold).any(axis=1)]\n",
    "\n",
    "    print(df_dsp_back_ho_norm.shape, df_dsp_back_ba_norm.shape)\n",
    "\n",
    "    # Calculate correlation and p-value matrices\n",
    "    num_host_genes = df_dsp_back_ho_norm.shape[0]\n",
    "    num_bacterial_genes = df_dsp_back_ba_norm.shape[0]\n",
    "    correlation_matrix = np.zeros((num_host_genes, num_bacterial_genes))\n",
    "    p_value_matrix = np.zeros((num_host_genes, num_bacterial_genes))\n",
    "\n",
    "    for i, host_gene in enumerate(df_dsp_back_ho_norm.index):\n",
    "        for j, bacterial_gene in enumerate(df_dsp_back_ba_norm.index):\n",
    "            correlation_matrix[i, j], p_value_matrix[i, j] = spearmanr(df_dsp_back_ho_norm.loc[host_gene], df_dsp_back_ba_norm.loc[bacterial_gene])\n",
    "\n",
    "    # Handle potential nan values in p_value_matrix\n",
    "    p_value_matrix = np.nan_to_num(p_value_matrix, nan=1.0)\n",
    "    \n",
    "    # Apply FDR correction\n",
    "    p_value_matrix = p_value_matrix.flatten()\n",
    "    _, fdr_corrected_p_values, _, _ = multipletests(p_value_matrix, method='fdr_bh')\n",
    "    fdr_corrected_p_values = fdr_corrected_p_values.reshape(correlation_matrix.shape)\n",
    "\n",
    "    # Save the matrices to the folder\n",
    "    np.savetxt(correlation_file, correlation_matrix, delimiter=\",\")\n",
    "    np.savetxt(p_value_file, p_value_matrix, delimiter=\",\")\n",
    "    np.savetxt(fdr_p_value_file, fdr_corrected_p_values, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set thresholds\n",
    "correlation_threshold = 0.7  # Adjust this value based on your desired correlation threshold\n",
    "alpha = 0.05  # Adjust this value based on your desired significance level\n",
    "\n",
    "# Identify significant interactions\n",
    "significant_interactions = np.where((np.abs(correlation_matrix) > correlation_threshold) & (fdr_corrected_p_values < alpha))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from adjustText import adjust_text\n",
    "\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# Create interaction network graph\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "interaction_network = nx.Graph()\n",
    "\n",
    "##########human ppi to add\n",
    "corr_host_genes = pd.DataFrame([x for x in list(interaction_network.nodes()) if x != 'rplL'])\n",
    "corr_host_genes.to_csv('correlation_norm_013124/corr_host_genes.txt', index=False, header=False)\n",
    "\n",
    "# add ppi between host genes\n",
    "df = pd.read_excel('correlation_norm_013124/PPIs_corr_host/PPIData_1.xlsx')\n",
    "df2 = pd.read_excel('correlation_norm_013124/PPIs_corr_host/PPIData_2.xlsx')\n",
    "df3 = pd.read_excel('correlation_norm_013124/PPIs_corr_host/PPIData_3.xlsx')\n",
    "# combined\n",
    "df = df[df['Entrez GeneID A'].notnull()]\n",
    "df2 = df2[df2['Entrez GeneID A'].notnull()]\n",
    "df3 = df3[df3['Entrez GeneID A'].notnull()]\n",
    "\n",
    "df_human_ppi = pd.concat([df, df2, df3])\n",
    "df_human_ppi = df_human_ppi[df_human_ppi['Entrez GeneID A'] != df_human_ppi['Entrez GeneID B']]\n",
    "\n",
    "# change to lower case for gene names except for the first letter\n",
    "df_human_ppi['Symbol A'] = df_human_ppi['Symbol A'].apply(lambda x: x[0] + x[1:].lower())\n",
    "df_human_ppi['Symbol B'] = df_human_ppi['Symbol B'].apply(lambda x: x[0] + x[1:].lower())\n",
    "\n",
    "# Loop through the human-human PPI DataFrame to add each interaction\n",
    "for index, row in df_human_ppi.iterrows():\n",
    "    gene_a = row['Symbol A']\n",
    "    gene_b = row['Symbol B']\n",
    "    # Add an edge for each human-human PPI\n",
    "    interaction_network.add_edge(gene_a, gene_b, type='host_host')\n",
    "\n",
    "###################\n",
    "\n",
    "# Define host_genes and bacterial_genes\n",
    "host_genes = df_dsp_back_ho_norm.index\n",
    "bacterial_genes = df_dsp_back_ba_norm.index\n",
    "\n",
    "# Find the index for 'rplL' in bacterial_genes\n",
    "rplL_index = np.where(bacterial_genes == 'rplL')[0][0]  # Assuming 'rplL' exists in bacterial_genes\n",
    "\n",
    "# Extract correlations and FDR values for interactions between 'Pmaip1', 'Tnf', 'Trim13' and 'rplL'\n",
    "target_genes = ['Pmaip1', 'Tnf', 'Trim13']\n",
    "for gene in target_genes:\n",
    "    if gene in host_genes:\n",
    "        gene_idx = np.where(host_genes == gene)[0][0]  # Get the host gene index\n",
    "        if (gene_idx, rplL_index) in zip(*significant_interactions):\n",
    "            rho_value = correlation_matrix[gene_idx, rplL_index]\n",
    "            fdr_value = fdr_corrected_p_values[gene_idx, rplL_index]\n",
    "            print(f\"Interaction between {gene} and rplL: rho = {rho_value:.3f}, FDR = {fdr_value:.2e}\")\n",
    "\n",
    "# Add edges and nodes with links\n",
    "linked_host_genes = set()\n",
    "linked_bacterial_genes = set()\n",
    "\n",
    "for i, j in zip(*significant_interactions):\n",
    "    host_gene = host_genes[i]\n",
    "    bacterial_gene = bacterial_genes[j]\n",
    "    if bacterial_gene == 'rplL':\n",
    "        # weight = correlation_matrix[i, j]\n",
    "        interaction_network.add_edge(host_gene, bacterial_gene, type='host_rplL')\n",
    "        linked_host_genes.add(host_gene)\n",
    "        linked_bacterial_genes.add(bacterial_gene)\n",
    "\n",
    "# Create a dictionary to map each linked bacterial gene to a unique color\n",
    "linked_bacterial_gene_colors = {bacterial_gene: plt.cm.get_cmap('tab10')(i % 20) for i, bacterial_gene in enumerate(linked_bacterial_genes)}\n",
    "\n",
    "# Set edge colors\n",
    "edge_colors = []\n",
    "for u, v, data in interaction_network.edges(data=True):\n",
    "    if data.get('type') == 'host_host':\n",
    "        edge_colors.append('orange')  # Human-human PPIs in red\n",
    "    elif data.get('type') == 'host_rplL':\n",
    "        # Retain original coloring logic for other interactions\n",
    "        edge_colors.append('steelblue')\n",
    "# Calculate node degrees\n",
    "node_degrees = dict(interaction_network.degree())\n",
    "\n",
    "# Set node sizes based on degree\n",
    "# node_sizes = [10 * node_degrees[node] for node in interaction_network.nodes()]\n",
    "node_sizes = [30  for node in interaction_network.nodes()]\n",
    "\n",
    "# Set node colors\n",
    "node_colors = [\"skyblue\" if n in linked_host_genes else \"lightcoral\" for n in interaction_network.nodes()]\n",
    "\n",
    "# Custom layout function to evenly distribute nodes with equal spacing\n",
    "def equal_spacing_layout(G, linked_bacterial_genes):\n",
    "    pos = nx.bipartite_layout(G, linked_bacterial_genes)\n",
    "\n",
    "    # Calculate the number of host and bacterial nodes\n",
    "    num_host_nodes = len(G.nodes) - len(linked_bacterial_genes)\n",
    "    num_bacterial_nodes = len(linked_bacterial_genes)\n",
    "\n",
    "    # Assign equal spacing for bacterial nodes\n",
    "    bacterial_spacing = 1 / (num_bacterial_nodes + 1)\n",
    "    for i, node in enumerate(sorted(linked_bacterial_genes, key=lambda x: G.degree(x), reverse=False)):\n",
    "        x, _ = pos[node]\n",
    "        pos[node] = (x, (i + 1) * bacterial_spacing)\n",
    "\n",
    "    # Assign equal spacing for host nodes\n",
    "    host_spacing = 1 / (num_host_nodes + 1)\n",
    "    for i, node in enumerate(sorted(G.nodes - linked_bacterial_genes, key=lambda x: G.degree(x), reverse=False)):\n",
    "        x, _ = pos[node]\n",
    "        pos[node] = (x, (i + 1) * host_spacing)\n",
    "\n",
    "    return pos\n",
    "\n",
    "# Visualize the network\n",
    "pos = nx.kamada_kawai_layout(interaction_network, scale=1)\n",
    "nx.draw(interaction_network, pos, node_color=node_colors, edge_color=edge_colors, width=2, node_size=node_sizes, alpha= 0.4)\n",
    "\n",
    "# Add labels with adjusted positions to prevent overlap\n",
    "texts = []\n",
    "for node, (x, y) in pos.items():\n",
    "    if node == 'rplL':  # Only label 'rplL'\n",
    "        texts.append(plt.text(x, y, node, ha='center', va='center', fontsize=16))\n",
    "    elif node in linked_host_genes:\n",
    "        texts.append(plt.text(x, y, node, ha='center', va='center', fontsize=12))\n",
    "\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle=\"-\", color='grey', lw=0.5))\n",
    "\n",
    "plt.title('Host-Bacteria Gene Expression Corr_Net (rho > '  + str(correlation_threshold)  + '; FDR < ' + str(alpha) + ')', fontsize = 13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different folds represent different biological replicates\n",
    "meta_wt = meta[meta.Group.str.contains('WT_PA')]\n",
    "meta_wt.loc[ meta_wt['Scan name'].str.startswith('A'), 'mouse_id'] = 'A'\n",
    "meta_wt.loc[ meta_wt['Scan name'].str.startswith('B'), 'mouse_id'] = 'B'\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "sns.set(font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def evaluate_features_performance(df_dsp_back_ho_norm_new, df_dsp_back_ba_norm_new, top_n_features_list, genes_of_interest, meta_wt):\n",
    "\n",
    "    # kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    model_name = \"Ridge Regression\"\n",
    "    best_r2_score = -np.inf\n",
    "    best_n_features = None\n",
    "    r2_scores_list = []\n",
    "\n",
    "    # Hyperparameters for Ridge Regression\n",
    "    params = {\n",
    "        \"alpha\": [1e-08, 1e-07, 1e-6, 1e-5, 1e-4, 1e-3, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "\n",
    "    gkf = GroupKFold(n_splits=2)\n",
    "    groups = meta_wt.loc[df_dsp_back_ho_norm_new.index, 'mouse_id']\n",
    "\n",
    "    for n in top_n_features_list:\n",
    "        file_path = f\"feat_imp/{model_name.replace(' ', '_').lower()}_{genes_of_interest.replace('/', '_')}_feature_importances.csv\"\n",
    "        feat_imp_df = pd.read_csv(file_path)\n",
    "        feat_imp_df['Importance'] = np.abs(feat_imp_df['Importance'])\n",
    "        top_features = feat_imp_df.sort_values(by='Importance', ascending=False).head(n)['Feature'].values\n",
    "\n",
    "        r2_scores = []\n",
    "\n",
    "        # Create Ridge model and GridSearchCV\n",
    "        ridge = Ridge()\n",
    "        grid = GridSearchCV(ridge, params, cv=gkf, scoring=\"neg_mean_absolute_error\",  n_jobs=-1)\n",
    "        grid.fit(df_dsp_back_ho_norm_new[top_features], df_dsp_back_ba_norm_new, groups=groups,)\n",
    "        best_model = grid.best_estimator_\n",
    "\n",
    "        for train_idx, test_idx in gkf.split(df_dsp_back_ho_norm_new, groups=groups):\n",
    "            X_train, X_test = df_dsp_back_ho_norm_new.iloc[train_idx][top_features], df_dsp_back_ho_norm_new.iloc[test_idx][top_features]\n",
    "            y_train, y_test = df_dsp_back_ba_norm_new.iloc[train_idx], df_dsp_back_ba_norm_new.iloc[test_idx]\n",
    "\n",
    "            best_model.fit(X_train, y_train)\n",
    "\n",
    "            pred = best_model.predict(X_test)\n",
    "            r2_scores.append(r2_score(y_test, pred))\n",
    "\n",
    "        # print best model's parameters\n",
    "        # print(f\"Best model's parameters: {best_model.get_params()}; \" + f\"R²: {np.mean(r2_scores):.3f}; Top {n} features\")\n",
    "\n",
    "        avg_r2_score = np.mean(r2_scores)\n",
    "        r2_scores_list.append(avg_r2_score)\n",
    "        if avg_r2_score > best_r2_score:\n",
    "            best_r2_score = avg_r2_score\n",
    "            best_n_features = n\n",
    "\n",
    "    return best_n_features, r2_scores_list\n",
    "\n",
    "def run_ML_top_features(df_dsp_back_ho_norm_new, df_dsp_back_ba_norm_new, genes_of_interest, scatter_size, top_n_features, axs, meta_wt):\n",
    "\n",
    "    gkf = GroupKFold(n_splits=2)\n",
    "    groups = meta_wt.loc[df_dsp_back_ho_norm_new.index, 'mouse_id']\n",
    "\n",
    "    # kfold = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    models = [Ridge()]\n",
    "    model_names = [\"Ridge Regression\"]\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        model_name = model_names[i]\n",
    "\n",
    "        # Define hyperparameters for Ridge Regression\n",
    "        params = {\n",
    "            \"alpha\": [1e-08, 1e-07, 1e-6, 1e-5, 1e-4, 1e-3, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        }\n",
    "\n",
    "        # Grid search to find best parameters\n",
    "        ridge = Ridge()\n",
    "        grid = GridSearchCV(ridge, params, cv=gkf, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "        grid.fit(df_dsp_back_ho_norm_new[top_n_features], df_dsp_back_ba_norm_new,groups=groups )\n",
    "        best_model = grid.best_estimator_\n",
    "\n",
    "        # best_model = pickle.load(open(f'best_models_013024/{model_name.replace(\" \", \"_\").lower()}_{genes_of_interest.replace(\"/\", \"_\")}.pkl', 'rb'))\n",
    "        \n",
    "        scores = []\n",
    "        r2_scores = []\n",
    "        ev_scores = []\n",
    "\n",
    "        for j, (train, test) in enumerate(gkf.split(df_dsp_back_ho_norm_new[top_n_features], groups=groups)):\n",
    "            X_train, X_test = (\n",
    "                df_dsp_back_ho_norm_new[top_n_features].iloc[train],\n",
    "                df_dsp_back_ho_norm_new[top_n_features].iloc[test],\n",
    "            )\n",
    "            y_train, y_test = (\n",
    "                df_dsp_back_ba_norm_new.iloc[train],\n",
    "                df_dsp_back_ba_norm_new.iloc[test],\n",
    "            )\n",
    "\n",
    "            best_model.fit(X_train, y_train)\n",
    "            # print best model's parameters\n",
    "            pred = best_model.predict(X_test)\n",
    "\n",
    "            mae = mean_absolute_error(y_test, pred)\n",
    "            scores.append(mae)\n",
    "            r2_scores.append(r2_score(y_test, pred))\n",
    "            ev_scores.append(explained_variance_score(y_test, pred))\n",
    "\n",
    "            axs.scatter(\n",
    "                y_test,\n",
    "                pred,\n",
    "                alpha=0.6,\n",
    "                label=\"Host_Fold {}; MAE: {:.2f}\".format(j + 1, mae),\n",
    "                s=scatter_size,\n",
    "            )\n",
    "        \n",
    "        print(np.mean(r2_scores))\n",
    "\n",
    "        axs.set_title(\n",
    "            \"{} - Avg MAE: {:.3f}, Avg EVS: {:.3f}; R²: {:.3f}; Top {} feat\".format(\n",
    "                'RR', np.mean(scores), np.mean(ev_scores), np.mean(r2_scores), len(top_n_features)\n",
    "            )\n",
    "        )\n",
    "        axs.set_xlabel(\"True Values (\" + genes_of_interest + \")\", size=16)\n",
    "        axs.set_ylabel(\"Predictions (\" + genes_of_interest + \")\", size=16)\n",
    "        axs.legend()\n",
    "        axs.plot(\n",
    "            [df_dsp_back_ba_norm_new.min(), df_dsp_back_ba_norm_new.max()],\n",
    "            [df_dsp_back_ba_norm_new.min(), df_dsp_back_ba_norm_new.max()],\n",
    "            \"k--\",\n",
    "        )\n",
    "\n",
    "n_rows = 1\n",
    "n_cols = 1\n",
    "\n",
    "genes = [\"rplL\"]\n",
    "n_features_list = [10, 20, 30, 80, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000,\n",
    "                   2000, 3000, 4000, 5000, 7500, 10000, 11000, 12000, 15000, 19962]  # Adjust as needed\n",
    "\n",
    "selected_rois = df_dsp_back_ba.loc['rplL'][\n",
    "        df_dsp_back_ba.loc['rplL'] >= 0\n",
    "    ].index.tolist()\n",
    "    \n",
    "selected_rois = list(set(meta[meta.Group.isin(['WT_PA'])].index.tolist()) & set(selected_rois))\n",
    "\n",
    "# df_dsp_back_ba_norm_new = df_dsp_back_ba[selected_rois].loc[genes_of_interest]\n",
    "df_dsp_back_ba_norm_new = np.log2(df_dsp_back_ba[selected_rois].loc['rplL'] + 1)\n",
    "\n",
    "print(\"bac_shape:\", df_dsp_back_ba_norm_new.shape)\n",
    "\n",
    "# normalized \"df_dsp_back_ho_select\" using StandardScaler >> df_dsp_back_ho_norm_new (Host gene expression)\n",
    "# df_dsp_back_ho_norm_new's shape is (43, 19962), sharing same columns with \"df_dsp_back_ba_norm_new\"\n",
    "scaler = StandardScaler()\n",
    "df_dsp_back_ho_select = df_dsp_back_ho[df_dsp_back_ba_norm_new.index]\n",
    "\n",
    "df_dsp_back_ho_norm_new = scaler.fit_transform(df_dsp_back_ho_select.T)\n",
    "df_dsp_back_ho_norm_new = pd.DataFrame(\n",
    "    df_dsp_back_ho_norm_new,\n",
    "    index=df_dsp_back_ho_select.columns,\n",
    "    columns=df_dsp_back_ho_select.index,\n",
    ")\n",
    "\n",
    "# Evaluate the performance of features\n",
    "best_n, r2_scores = evaluate_features_performance(df_dsp_back_ho_norm_new, df_dsp_back_ba_norm_new, n_features_list, 'rplL', meta_wt)\n",
    "\n",
    "# Find the number of features where R2 first exceeds 0.8\n",
    "above_08_indices = [i for i, r2 in enumerate(r2_scores) if r2 > 0.8]\n",
    "if above_08_indices:\n",
    "    index_first_above_08 = above_08_indices[0]\n",
    "    n_features_first_above_08 = n_features_list[index_first_above_08]\n",
    "    r2_first_above_08 = r2_scores[index_first_above_08]\n",
    "else:\n",
    "    index_first_above_08 = None\n",
    "    n_features_first_above_08 = None\n",
    "    r2_first_above_08 = None\n",
    "\n",
    "# Find the number of features with the highest R2 score\n",
    "max_r2 = max(r2_scores)\n",
    "index_max_r2 = r2_scores.index(max_r2)\n",
    "best_n_features = n_features_list[index_max_r2]\n",
    "\n",
    "# Plotting R2 scores vs Number of features\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(n_features_list, r2_scores, marker='x', label='R² Score', alpha=0.6)\n",
    "\n",
    "# Highlight the maximum point\n",
    "plt.scatter(best_n_features, max_r2, color='red', s=50, marker='o',\n",
    "             label=f'Max R²={max_r2:.3f} with {best_n_features} features')\n",
    "\n",
    "# Highlight the first point where R2 is above 0.8\n",
    "if index_first_above_08 is not None:\n",
    "    plt.scatter(n_features_first_above_08, r2_first_above_08, color='blue', s=50, marker='o',\n",
    "                label=f'First R²>0.8 at {n_features_first_above_08} features')\n",
    "\n",
    "plt.xlabel('Number of Features (Mouse Genes)')\n",
    "plt.ylabel('Average R² Score')\n",
    "plt.title('R² Score vs Number of Mouse Genes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "ranked_features_all = []\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(5, 5))\n",
    "\n",
    "for idx, genes_of_interest in enumerate(tqdm(genes)):\n",
    "    selected_rois = df_dsp_back_ba.loc[genes_of_interest][\n",
    "        df_dsp_back_ba.loc[genes_of_interest] >= 0\n",
    "    ].index.tolist()\n",
    "    selected_rois = list(set(meta[meta.Group.isin(['WT_PA'])].index.tolist()) & set(selected_rois))\n",
    "    \n",
    "    # df_dsp_back_ba_norm_new = df_dsp_back_ba[selected_rois].loc[genes_of_interest]\n",
    "    df_dsp_back_ba_norm_new = np.log2(df_dsp_back_ba[selected_rois].loc[genes_of_interest] + 1)\n",
    "    print(\"bac_shape:\", df_dsp_back_ba_norm_new.shape)\n",
    "\n",
    "    # normalized \"df_dsp_back_ho_select\" using StandardScaler >> df_dsp_back_ho_norm_new (Host gene expression)\n",
    "    # df_dsp_back_ho_norm_new's shape is (43, 19962), sharing same columns with \"df_dsp_back_ba_norm_new\"\n",
    "    scaler = StandardScaler()\n",
    "    df_dsp_back_ho_select = df_dsp_back_ho[df_dsp_back_ba_norm_new.index]\n",
    "    \n",
    "    df_dsp_back_ho_norm_new = scaler.fit_transform(df_dsp_back_ho_select.T)\n",
    "    df_dsp_back_ho_norm_new = pd.DataFrame(\n",
    "        df_dsp_back_ho_norm_new,\n",
    "        index=df_dsp_back_ho_select.columns,\n",
    "        columns=df_dsp_back_ho_select.index,\n",
    "    )\n",
    "    print(\"ho_shape:\", df_dsp_back_ho_norm_new.shape)\n",
    "    \n",
    "    rf_model_name = 'Ridge Regression'\n",
    "    avg_feat_imp_file = f\"feat_imp/{rf_model_name.replace(' ', '_').lower()}_{genes_of_interest.replace('/', '_')}_feature_importances.csv\"\n",
    "    feat_imp_df = pd.read_csv(avg_feat_imp_file)\n",
    "    feat_imp_df['Importance'] = np.abs(feat_imp_df['Importance'])\n",
    "    ranked_features = feat_imp_df.sort_values(by='Importance', ascending=False).head(best_n)['Feature'].values\n",
    "    ranked_features_all.append(ranked_features)\n",
    "    run_ML_top_features(df_dsp_back_ho_norm_new, df_dsp_back_ba_norm_new, genes_of_interest, 40, ranked_features, axs, meta_wt)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
